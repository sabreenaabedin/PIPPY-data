dnn = c('actual default', 'predicted default'))
library(ipred)
mybag <-bagging(quality~.,data=wine_train)
pbag <-predict(mybag,wine_test[,-12])
mybag2 <- bagging(quality~.,data=wine,coob=TRUE)
library(randomForest)
model <- randomForest(quality ~ . , data = wine_train)
install.packages('gapminder')
library(gapminder)
str(gapminder)
rm(list = ls())
save.image("~/.RData")
summary(gapminder)
load("~/SYS4021/Spam/.RData")
library(forecast)
plot(spam.ts)
acf(spam.ts)
pg.ham<-spec.pgram(ham.ts,spans=9,demean=T,log='no')
pg.spam <- spec.pgram(spam.ts,spans=9, demean=T, log='no')
max.omega.spam <- pg.spam$freq[which(pg.spam$spec==max(pg.spam$spec))]
1/max.omega.spam
pg.ham<-spec.pgram(ham.ts,spans=9,demean=T,log='no')
spam.trend<-lm(spam.ts[1:(length(spam.ts))]~time.spam)
time.spam<-c(1:(length(spam.ts)))
spam.trend<-lm(spam.ts[1:(length(spam.ts))]~time.spam)
e.ts.spam <- ts(spam.trend$residuals)
plot(e.ts.spam)
acf(e.ts.spam)
pacf(e.ts.spam)
spam.auto <- auto.arima(e.ts.spam, trace=TRUE, stepwise = FALSE)
summary(spam.auto)
spam.arma11.whole <- arima(spam.ts, order=c(1,0,1))
spam.arima111 <- arima(e.ts.spam, order=c(1,1,1))
summary(spam.arima111)
AIC(spam.arima111)
ACF(spam.arima111)
acf(spam.arima111)
pacf(spam.arima111)
time.spam<-c(1:(length(spam.ts)))
spam.trend<-lm(spam.ts[1:(length(spam.ts))]~time.spam)
spam.trend<-lm(spam.ts[1:(length(spam.ts)-1)]~time.spam)
spam.trend<-lm(spam.ts[1:(length(spam.ts))]~time.spam)
summary(spam.trend)
e.ts.spam <- ts(spam.trend$residuals)
plot(e.ts.spam)
acf(e.ts.spam)
pacf(e.ts.spam)
spam.auto <- auto.arima(e.ts.spam, trace=TRUE, stepwise = FALSE)
summary(spam.auto)
time.spam<-c(1:(length(spam.ts))-7)
time.spam<-c(1:(length(spam.ts)-7))
spam.trend<-lm(spam.ts[1:(length(spam.ts))]~time.spam)
spam.trend<-lm(spam.ts[1:357]~time.spam)
e.ts.spam <- ts(spam.trend$residuals)
plot(e.ts.spam)
acf(e.ts.spam)
pacf(e.ts.spam)
spam.auto <- auto.arima(e.ts.spam, trace=TRUE, stepwise = FALSE)
summary(spam.auto)
spam.arima111 <- arima(e.ts.spam, order=c(1,1,1))
summary(spam.arima111)
acf(spam.arima111)
pacf(spam.arima111)
acf(spam.arima111$residuals)
pacf(spam.arima111$residuals)
spam.auto1 <- auto.arima(spam.ts[time.spam], trace = TRUE)
summary(spam.auto1)
acf(spam.auto$residuals)
pacf(spam.auto$residuals)
pg.spam <- spec.pgram(spam.ts,spans=9, demean=T, log='no')
max.omega.spam <- pg.spam$freq[which(pg.spam$spec==max(pg.spam$spec))]
1/max.omega.spam
max.omega.spam
acf(e.ts.spam)
par(mfrow=c(1,2))
acf(e.ts.spam, main="ACF of Residuals from spam.trend")
pacf(e.ts.spam,main="PACF of Residuals from spam.trend")
par(mfrow=c(1,1))
pg.spam <- spec.pgram(spam.ts,spans=9, demean=T, log='no')
?spec.pgram
pg.spam <- spec.pgram(spam.ts,spans=9, demean=T, log='no')
max.omega.spam <- pg.spam$freq[which(pg.spam$spec==max(pg.spam$spec))]
max.omega.spam
1/max.omega.spam
1/max.omega.ham
max.omega.ham<-pg.ham$freq[which(pg.ham$spec==max(pg.ham$spec))]
1/max.omega.ham
?tsdiag
tsdiag(spam.arma11,gof.lag=20)
spam.arma11 <- arima(e.ts.spam, order=c(1,0,1))
spam.ar1 <- arima(e.ts.spam, order=c(1,0,0))
spam.arima111 <- arima(e.ts.spam, order=c(1,1,1))
spam.auto <- auto.arima(e.ts.spam, trace=TRUE, stepwise = FALSE)
spam.auto.whole <- auto.arima(spam.ts, trace=TRUE)
spam.arma11.whole <- arima(spam.ts, order=c(1,0,1))
spam.auto1 <- auto.arima(spam.ts[time.spam], trace = TRUE)
tsdiag(spam.arma11,gof.lag=20)
tsdiag(spam.ar1,gof.lag=20)
tsdiag(spam.arima111,gof.lag=20)
tsdiag(spam.auto,gof.lag=20)
tsdiag(spam.auto1,gof.lag=20)
tsdiag(spam.auto, gof.lag=20)
tsdiag(spam.arima111,gof.lag=20)
tsdiag(spam.auto1,gof.lag=20)
load("~/DS4559/Dating/dating.RData")
adaboostRat <- boosting(dec~age_o+race_o+attr+sinc+intel+fun+amb+shar+like+met,data = dd1_train, boos=FALSE, mfinal=20, coeflearn='Freund')
library(adabag)
adaboostRat <- boosting(dec~age_o+race_o+attr+sinc+intel+fun+amb+shar+like+met,data = dd1_train, boos=FALSE, mfinal=20, coeflearn='Freund')
names(dd1_train)
adaboostRat <- boosting(dec~age_o+race_o+attr+sinc+intel+fun+amb+like+met,data = dd1_train, boos=FALSE, mfinal=20, coeflearn='Freund')
adaboostRat$importance[which(adaboostRat$importance>0)]
adaboostRat <- boosting(dec~age_o+race_o+attr+sinc+intel+fun+amb+met,data = dd1_train, boos=FALSE, mfinal=20, coeflearn='Freund')
adaboostRat <- boosting(dec~age_o+race_o+attr+sinc+intel+fun+amb+met,data = dd1_train, boos=FALSE, mfinal=20, coeflearn='Freund')
adaboostRat$importance[which(adaboostRat$importance>0)]
dd1_train$gender
dd1_trainM <- dd1_train[where(dd1_train$gender==1)]
dd1_trainM <- dd1_train[which(dd1_train$gender==1)]
dd1_trainM <- dd1_train %>% filter(gender == 1)
library(tidvyverse)
library(tidyverse)
dd1_trainM <- dd1_train %>% filter(gender == 1)
adaboostRatM <- boosting(dec~age_o+race_o+attr+sinc+intel+fun+amb+met,data = dd1_trainM, boos=FALSE, mfinal=20, coeflearn='Freund')
adaboostRatM$importance[which(adaboostRatM$importance>0)]
dd1_trainF <- dd1_train %>% filter(gender == 0)
adaboostRatF <- boosting(dec~age_o+race_o+attr+sinc+intel+fun+amb+met,data = dd1_trainF, boos=FALSE, mfinal=20, coeflearn='Freund')
adaboostRatF$importance[which(adaboostRatF$importance>0)]
dating_data %>% ggplot(aes(x=career_c,y=match, fill = factor(gender))) + geom_bar(stat = 'identity')
dating_data %>% ggplot(aes(x=career_c,y=match)) + geom_bar(stat = 'identity')
dating_data %>% ggplot(aes(x=career_c,y=match)) + geom_bar(stat = 'identity')+coord_flip()
dating_data %>% ggplot(aes(x=career_c,y=gender)) + geom_bar(stat = 'identity')+coord_flip()
dating_data %>% ggplot(aes(x=career_c,y=factor(gender))) + geom_bar(stat = 'identity')+coord_flip()
dating_data %>% ggplot(aes(x=career_c,y=factor(gender))) + geom_boxplot(stat = 'identity')+coord_flip()
dating_data %>% ggplot(aes(x=career_c,y=gender)) + geom_boxplot(stat = 'identity')+coord_flip()
dating_data %>% ggplot(aes(x=career_c,y=as.numeric(match))) + geom_boxplot(stat = 'identity')+coord_flip()
dating_data %>% ggplot(aes(x=career_c,y=as.numeric(match))) + geom_bar(stat = 'identity')+coord_flip()
dating_data %>% ggplot(aes(x=career_c,y=as.numeric(match), fill = as.factor(gender))) + geom_bar(stat = 'identity')+coord_flip()
dating_data %>% ggplot(aes(x=career_c,y=as.numeric(match), fill = as.factor(gender))) + geom_bar(stat = 'identity', position = 'dodge')+coord_flip()
apply(dating_data,2,pMiss)
##Let's see how much of the dating data is NA's
sum(is.na(dating_data))/(8378*195)*100
average(dating_data$attr)
mean(dating_data$attr)
mean(dating_data$attr, rm.na = TRUE)
mean(dating_data$attr, na.rm = TRUE)
mean(dating_data$attr_o, na.rm = TRUE)
dating_data %>% filter(gender == 0) %>% summarise(mean(attr))
dating_data %>% filter(gender == 0, !is.na(attr)) %>% summarise(mean(attr))
dating_data %>% filter(gender == 0, !is.na(attr)) %>% summarise(mean(attr_o))
dating_data %>% filter(gender == 0, !is.na(attr_o)) %>% summarise(mean(attr_o))
dating_data %>% filter(gender == 0, !is.na(attr3_1)) %>% summarise(mean(attr3_1))
dating_data %>% filter(gender == 1, !is.na(attr3_1)) %>% summarise(mean(attr3_1))
dating_data %>% filter(!is.na(attr3_1)) %>% summarise(mean(attr3_1))
dating_data %>% filter(!is.na(age)) %>% summarise(mean(age))
dating_data %>% filter(!is.na(race)) %>% summarise(race)
dating_data %>% filter(!is.na(race)) %>% select(race)
dating_data %>% filter(!is.na(race)) %>% summarize(sum(race))
dating_data %>% group_by(race) %>% filter(!is.na(race)) %>% summarize(sum(race))
dating_data %>% group_by(race) %>% filter(!is.na(race)) %>% summarize(sum(race)/sum(dating_data$race))
dating_data %>% group_by(race) %>% filter(!is.na(race)) %>% summarize(sum(race)/sum(dating_data$race, na.rm = TRUE))
dating_data %>% group_by(race) %>% filter(!is.na(race)) %>% summarize(sum(race)/sum(dating_data$race, na.rm = TRUE)*100)
############### Making our models better II: removing correlation ######################
## install a funky new package:
install.packages("corrplot")
corrAttr <- corr(dd1[,c('attr_o','age_o','race_o','attr','sinc','intel','fun','amb','met')])
library(corrPlot)
corrAttr <- corr(dd1[,c('attr_o','age_o','race_o','attr','sinc','intel','fun','amb','met')])
library(corrplot)
corrAttr <- corr(dd1[,c('attr_o','age_o','race_o','attr','sinc','intel','fun','amb','met')])
corrAttr <- cor(dd1[,c('attr_o','age_o','race_o','attr','sinc','intel','fun','amb','met')])
plot(corrAttr)
summary(adaboost1,order='hclust')
plot(corrAttr, order = 'hclust')
corrplot(corrAttr, order = 'hclust')
corrAttr <- cor(dd1[,c('attr_o','attr','sinc','intel','fun','amb','met')])
corrplot(corrAttr, order = 'hclust')
corrAttr <- cor(dd1[,c('attr_o','attr','sinc','intel','fun','amb','met','match')])
corrplot(corrAttr, order = 'hclust')
corrAttr <- cor(dd1[,c('attr_o','attr','sinc','intel','fun','amb','met','as.numeric(match)')])
corrAttr <- cor(dd1[,c('attr_o','attr','sinc','intel','fun','amb','met','dec')])
corrAttr <- cor(dd1[,c('attr_o','attr','sinc','intel','fun','amb','met','race_o','age_o')])
corrplot(corrAttr, order = 'hclust')
dd1_test[12]
names(dd1_test)
p1Rat <- predict(adaboostRat,dd1_test[-84])
RC1Rat <- roc(dd1_test$dec,p1Rat$prob[,2])
plot(RC1Rat, legacy.axes=TRUE)
auc(RC1Rat)
library(pROC)
p1Rat <- predict(adaboostRat,dd1_test[-84])
RC1Rat <- roc(dd1_test$dec,p1Rat$prob[,2])
plot(RC1Rat, legacy.axes=TRUE)
auc(RC1Rat)
adaboostRatM <- boosting(dec~age_o+race_o+attr+sinc+intel+fun+amb+met,data = dd1_trainM, boos=FALSE, mfinal=20, coeflearn='Freund')
adaboostRatM$importance[which(adaboostRatM$importance>0)]
p1RatM <- predict(adaboostRat,dd1_testM[-84])
RC1RatM <- roc(dd1_testM$dec,p1RatM$prob[,2])
plot(RC1RatM, legacy.axes=TRUE)
auc(RC1RatM)
dd1_testM <- dd1_test %>% filter(gender == 1)
dd1_testF <- dd1_test %>% filter(gender == 0)
p1RatM <- predict(adaboostRat,dd1_testM[-84])
RC1RatM <- roc(dd1_testM$dec,p1RatM$prob[,2])
plot(RC1RatM, legacy.axes=TRUE)
auc(RC1RatM)
p1RatF <- predict(adaboostRat,dd1_testF[-84])
RC1RatF <- roc(dd1_testF$dec,p1RatF$prob[,2])
plot(RC1RatF, legacy.axes=TRUE)
auc(RC1RatF)
plot(RC1Rat, legacy.axes = TRUE)
lines(RC1RatM)
lines(RC1RatM, col = 'Blue')
lines(RC1RatF, col = 'Orange')
plot(RC1Rat, legacy.axes = TRUE, col = 'Green')
lines(RC1RatM, col = 'Blue')
lines(RC1RatF, col = 'Orange')
corrAttr <- cor(dd1[,c('attr_o','attr','sinc','intel','fun','amb','met','race','age')])
corrplot(corrAttr, order = 'hclust')
adaboostRat$importance[which(adaboostRat$importance>0)]
##Males and Females
adaboostRat <- boosting(dec~age+race_o+attr+sinc+intel+fun+amb+met,data = dd1_train, boos=FALSE, mfinal=20, coeflearn='Freund')
##Males and Females
adaboostRat <- boosting(dec~age+race+attr+sinc+intel+fun+amb+met,data = dd1_train, boos=FALSE, mfinal=20, coeflearn='Freund')
adaboostRat$importance[which(adaboostRat$importance>0)]
##Just males
adaboostRatM <- boosting(dec~age+race+attr+sinc+intel+fun+amb+met,data = dd1_trainM, boos=FALSE, mfinal=20, coeflearn='Freund')
##Males and Females
adaboostRat <- boosting(dec~age_o+race_o+attr+sinc+intel+fun+amb+met,data = dd1_train, boos=FALSE, mfinal=20, coeflearn='Freund')
adaboostRat$importance[which(adaboostRat$importance>0)]
adaboostRatM$importance[which(adaboostRatM$importance>0)]
##Just males
adaboostRatM <- boosting(dec~age_o+race_o+attr+sinc+intel+fun+amb+met,data = dd1_trainM, boos=FALSE, mfinal=20, coeflearn='Freund')
adaboostRatF$importance[which(adaboostRatF$importance>0)]
install.packages('feedeR')
library(feedeR)
feed.extract('https://www.amazon.com/gp/rss/bestsellers/beauty/ref=zg_bs_beauty_rsslink')
am_beauty <- feed.extract('https://www.amazon.com/gp/rss/bestsellers/beauty/ref=zg_bs_beauty_rsslink')
am_beaty$items
am_beauty$items
am_beauty$items$title
library(feedeR)
am_beauty <- feed.extract('https://www.amazon.com/gp/rss/bestsellers/beauty/11060451/ref=zg_bs_11060451_rsslink')
am_beauty$items$title
am_beauty$items
am_beauty$items$rank <- substring(am_beauty$items$title, 2,3)
am_beauty$items$rank <- gsub(":","",am_beauty$items$rank)
am_beauty$items$rank <- as.integer(am_beauty$items$rank)
am_beauty$items$title
for (i in 1:10){
am_beauty$items$name[i] <- substring(am_beauty$items$title[i], 5, nchar(am_beauty$items$title[i]))
}
am_beauty$items$name
ranks <- data.frame(am_beauty$items$name, am_beauty$items$rank, am_beauty$items$date)
ranks
colnames(ranks) = c('Name','Rank', 'Date')
ranks
write.csv(ranks, file = 'Ranks3.csv')
for (i in 1:10){
am_beauty$items$name[i] <- substring(am_beauty$items$title[i], 5, nchar(am_beauty$items$title[i]))
}
words <- strsplit(ranks$Name, ' ')
ranks
typeof(ranks$Name)
ranks$Name
ambeauty$item$name[i]
am_beauty$items$name[1]
typeof(am_beauty$items$name)
words <- strsplit(am_beauty$items$name[1], ' ')
words
x <- c(as = "asfef", qu = "qwerty", "yuiop[", "b", "stuff.blah.yech")
strsplit(x, "e")
y<-strsplit(x, "e")
words[2]
words[,3]
words[1]
words[[2]]
am_beauty$items$title$getText()
getText(am_beauty$items$title)
library(base64enc)
library(ROAuth)
require(RCurl)
library(stringr)
library(ggmap)
library(dplyr)
library(plyr)
library(tm)
library(wordcloud)
getText(am_beauty$items$title)
am_beauty$items$title$getText()
?twitteR$getText()
?getText
??getText
??twitteR::getText
library(twitteR)
# get the text
some_txt = sapply(some_tweets, function(x) x$getText())
# get the text
some_txt = sapply(am_beauty$items$name, function(x) x$getText())
am_beauty$items$name
typeof(am_beauty$items$name)
words <- strsplit(am_beauty$items$name, ' ')
str(words)
setwd('C:/Users/Student/Documents/Capstone')
write.csv(ranks, file = 'Ranks3.csv')
ranks1 <- read.csv("Ranks3.csv")
ranks1 <- read.csv("Ranks1.csv")
ranks1 <- read.csv("Ranks.csv")
ranks2<- read.csv('Ranks2.csv')
ranks3 <- read.csv('Ranks3.csv')
ranks <- rbind(ranks1, ranks2, ranks3)
write.csv(rankTot, 'OverallRankingsOverTime.csv')
rankTot <- rbind(ranks1, ranks2, ranks3)
write.csv(rankTot, 'OverallRankingsOverTime.csv')
ranks <- data.frame(am_beauty$items$name, am_beauty$items$rank, am_beauty$items$date)
colnames(ranks) = c('Name','Rank', 'Date')
rankTot$words <- as.character(rankTot$Name)
rankTot$words
rank_words <- rankTot %>% tidytext::unnest_tokens(word, words) %>% mutate(Date, Rank)
typeof(rank_words$Date)
rank_words$Date <- as.Date(rank_words$Date)
rankTot$Date
max(rank_words$Date)
rank_w <- rank_words %>% inner_join(by = c(word='word'))
?inner_join
count(rank_words, word)
count(rank_words, 'word')
list1 <- rep(1,length(rank_words))
list1 <- rep(1,numrow(rank_words))
list1 <- rep(1,nrow(rank_words))
rank_words <- cbind(rank_words, list1)
aggregate(rank_words$list1, by = list(Category = rank_words$word), fun = SUM)
aggregate(rank_words$list1, by = list(Category = rank_words$word), FUN=sum)
rank_words <- rankTot %>% tidytext::unnest_tokens(word, words) %>% mutate(Date, Rank)
rank_words$Date <- as.Date(rank_words$Date)
word_counts <- count(rank_words, 'word')
rank_words <- merge(rank_words, word_counts[,c('word','freq')],by='word')
word_counts <- merge(word_counts, rank_words[,c('word','Date')],by='word')
min(rank_words$Date %>% filter(word == 'viva'))
min(rank_words %>% filter(word == 'viva') %>% select(Date))
min(rank_words$Date)
(rank_words %>% filter(word == 'viva') %>% select(Date))
typeof(rank_words %>% filter(word == 'viva') %>% select(Date))
(rank_words %>% filter(word == 'viva') %>% select(Date))$Date
min(rank_words %>% filter(word == 'viva') %>% select(Date))$Date)
min((rank_words %>% filter(word == 'viva') %>% select(Date))$Date)
word_counts$minDate <- rep(,nrow(word_counts))
word_counts$minDate <- rep("",nrow(word_counts))
word_counts$minDate[i] <- ((rank_words %>% filter(word == w) %>% select(Date))$Date)
for (i in nrow(word_counts)){
w = word_counts$word[i]
word_counts$minDate[i] <- ((rank_words %>% filter(word == w) %>% select(Date))$Date)
}
word_counts$minDate[i] <- min((rank_words %>% filter(word == w) %>% select(Date))$Date)
for (i in nrow(word_counts)){
w = word_counts$word[i]
word_counts$minDate[i] <- min((rank_words %>% filter(word == w) %>% select(Date))$Date)
}
word_counts$minDate[i]
word_counts$minDate[i] <- as.Date(min((rank_words %>% filter(word == w) %>% select(Date))$Date))
for (i in nrow(word_counts)){
w = word_counts$word[i]
word_counts$minDate[i] <- as.Date(min((rank_words %>% filter(word == w) %>% select(Date))$Date))
word_counts$minDate[i]
}
for (i in nrow(word_counts)){
w = word_counts$word[i]
word_counts$minDate[i] <- as.Date(min((rank_words %>% filter(word == w) %>% select(Date))$Date))
print(word_counts$minDate[i])
}
for (i in nrow(word_counts)){
i
w = word_counts$word[i]
word_counts$minDate[i] <- as.Date(min((rank_words %>% filter(word == w) %>% select(Date))$Date))
}
nrow(word_counts)
for (i in 1:nrow(word_counts))){
w = word_counts$word[i]
word_counts$minDate[i] <- as.Date(min((rank_words %>% filter(word == w) %>% select(Date))$Date))
}
for (i in 1:nrow(word_counts)){
w = word_counts$word[i]
word_counts$minDate[i] <- as.Date(min((rank_words %>% filter(word == w) %>% select(Date))$Date))
}
as.Date(min((rank_words %>% filter(word == w) %>% select(Date))$Date))
word_counts$minDate <- rep(as.Date("2018-02-13"),nrow(word_counts))
for (i in 1:nrow(word_counts)){
w = word_counts$word[i]
word_counts$minDate[i] <- as.Date(min((rank_words %>% filter(word == w) %>% select(Date))$Date))
}
table(word_counts$minDate)
rank_words <- merge(rank_words, word_counts[,c('word','freq','minDate')],by='word')
word_counts <- merge(word_counts, rank_words[,c('word','Date','Rank')],by='word')
word_counts$minDate <- rep(as.Date("2018-02-13"),nrow(word_counts))
for (i in 1:nrow(word_counts)){
w = word_counts$word[i]
word_counts$minDate[i] <- as.Date(min((rank_words %>% filter(word == w) %>% select(Date))$Date))
}
word_counts <- merge(word_counts, rank_words[,c('word','Date')],by='word')
word_counts <- count(rank_words, 'word')
word_counts <- merge(word_counts, rank_words[,c('word','Date')],by='word')
word_counts$minDate <- rep(as.Date("2018-02-13"),nrow(word_counts))
for (i in 1:nrow(word_counts)){
w = word_counts$word[i]
word_counts$minDate[i] <- as.Date(min((rank_words %>% filter(word == w) %>% select(Date))$Date))
}
rank_words <- rankTot %>% tidytext::unnest_tokens(word, words) %>% mutate(Date, Rank)
rank_words$Date <- as.Date(rank_words$Date)
max(rank_words$Date)
word_counts <- count(rank_words, 'word')
word_counts <- merge(word_counts, rank_words[,c('word','Date')],by='word')
word_counts$minDate <- rep(as.Date("2018-02-13"),nrow(word_counts))
for (i in 1:nrow(word_counts)){
w = word_counts$word[i]
word_counts$minDate[i] <- as.Date(min((rank_words %>% filter(word == w) %>% select(Date))$Date))
}
write.csv(word_counts, 'WordCountRanksAmazon.csv')
rank_words <- merge(rank_words, word_counts[,c('word','freq','minDate')],by='word')
rank_words <- rankTot %>% tidytext::unnest_tokens(word, words) %>% mutate(Date, Rank)
rank_words$Date <- as.Date(rank_words$Date)
rank_words <- merge(rank_words, word_counts[,c('word','minDate')],by='word')
rank_words <- rankTot %>% tidytext::unnest_tokens(word, words) %>% mutate(Date, Rank)
rank_words$Date <- as.Date(rank_words$Date)
rank_words <- merge(rank_words, word_counts[,c('word','freq')],by='word')
rank_words <- merge(rank_words, word_counts[,c('word')],by='word')
rank_words <- rankTot %>% tidytext::unnest_tokens(word, words) %>% mutate(Date, Rank)
rank_words$Date <- as.Date(rank_words$Date)
rank_words <- merge(rank_words, word_counts[,c('word')],by='word')
rank_words <- merge(rank_words,word_counts[,c('word','freq')],by='word')
rank_words <-rankTot %>% tidytext::unnest_tokens(word, words) %>% mutate(Date, Rank)
rank_words$Date <- as.Date(rank_words$Date)
word_counts <- count(rank_words, 'word')
rank_words <- merge(rank_words,word_counts[,c('word','freq')],by='word')
write.csv(rank_words, 'rank_wordsAmazon.csv')
max(word_counts$freq)
rankTot$words <- as.character(rankTot$Name)
rankTot$words = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", rankTot$words)
# remove at people
rankTot$words = gsub("@\\w+", "", rankTot$words)
# remove punctuation
rankTot$words = gsub("[[:punct:]]", "", rankTot$words)
# remove numbers
rankTot$words = gsub("[[:digit:]]", "", rankTot$words)
# remove html links
rankTot$words = gsub("http\\w+", "", rankTot$words)
# remove unnecessary spaces
rankTot$words = gsub("[ \t]{2,}", "", rankTot$words)
rankTot$words = gsub("^\\s+|\\s+$", "", rankTot$words)
rankTot$words = gsub('???','',rankTot$words)
rankTot$words = gsub('etc','',rankTot$words)
rank_words <-rankTot %>% tidytext::unnest_tokens(word, words) %>% mutate(Date, Rank)
rank_words$Date <- as.Date(rank_words$Date)
max(rank_words$Date)
word_counts <- count(rank_words, 'word')
rank_words <- merge(rank_words,word_counts[,c('word','freq')],by='word')
write.csv(rank_words, 'rank_wordsAmazon.csv')
word_counts <- merge(word_counts, rank_words[,c('word','Date')],by='word')
word_counts$minDate <- rep(as.Date("2018-02-13"),nrow(word_counts))
for (i in 1:nrow(word_counts)){
w = word_counts$word[i]
word_counts$minDate[i] <- as.Date(min((rank_words %>% filter(word == w) %>% select(Date))$Date))
}
write.csv(word_counts, 'WordCountRanksAmazon.csv')
save.image("~/Capstone/AmazonRankData.RData")
load("~/Capstone/AmazonRankData.RData")
rank_words
rank_words %>% group_by(word) %>% summary()
library(feedeR)
library(base64enc)
library(ROAuth)
require(RCurl)
library(stringr)
library(ggmap)
library(dplyr)
library(plyr)
library(tm)
library(wordcloud)
rank_words %>% group_by(word) %>% summary()
rank_words %>% group_by(word) %>% summarize(rank)
rank_words %>% group_by(word) %>% summarize(mean(rank))
rank_words %>% group_by(word) %>% (avg = mean(rank))
rank_words %>% group_by(word) %>% summarize(avg = mean(rank))
rank_words %>% group_by(word) %>% summarize(avg = mean(as.Integer(rank)))
rank_words %>% group_by(word) %>% summarize(avg = mean(as.integer(rank)))
typeof(rank_words$Rank)
rank_words %>% group_by(word) %>% summarize(avg = mean((Rank)))
rank_words$word
rank_words %>% group_by(word)# %>% summarize(avg = mean((Rank)))
rank_words %>% group_by(word) %>% summarize(avg = mean((Rank)))
rank_words %>% group_by(word) %>% summarize(mean((Rank)))
rank_words %>% dplyr::group_by(word) %>% dplyr::summarize(mean((Rank)))
AveWordRank <- rank_words %>% dplyr::group_by(word) %>% dplyr::summarize(mean((Rank)))
AveWordRank <- rank_words %>% dplyr::group_by(word) %>% dplyr::mutate((freq))
AveWordRank
AveWordRank <- rank_words %>% dplyr::group_by(word) %>% dplyr::summarize(mean((Rank)))
AveWordRank
AveWordRank <- rank_words %>% dplyr::group_by(word) %>% dplyr::summarize(mean((Rank))) %>% dplyr::mutate((freq))
AveWordRank <- rank_words %>% dplyr::group_by(word) %>% dplyr::summarize(mean((Rank))) %>% dplyr::mutate((rank_words$freq))
AveWordRank <- rank_words %>% dplyr::group_by(word) %>% dplyr::summarize(mean((Rank)))
?transmute
AveWordRank$freq <- rank_words %>% dplyr::group_by(word) %>% dplyr::mutate((rank_words$freq))
AveWordRank$freq <- rank_words %>% dplyr::group_by(word) %>% dplyr::mutate((freq))
AveWordRank <- merge(AveWordRank,word_counts[,c('word','freq')],by='word')
AveWordRank
AveWordRank <- AveWordRank[!duplicated(AveWordRank),]
AveWordRank
write.csv(AveWordRank, 'aveWordRank.csv')
load("~/Capstone/blogs.RData")
reddit <- read.table('redditData.txt')
setwd('~/GitHub/PIPPY-data')
reddit <- read.table('redditData.txt')
reddit <- read.delim('redditData.txt')
head(reddit)
View(reddit)
