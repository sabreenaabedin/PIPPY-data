wine_train <- wine_rand[4001:4898,]
wine_train <- wine_rand[1:4000,]
wine_test <- wine_rand[4001:4898,]
set.seed(234)
index_w1 <- sample(1:nrow(wine_train), 4000, replace = TRUE)
rr3 <- iris[index2, ]
m3 <- C5.0(x=rr3[,-13], y=rr3$output,earlyStopping = FALSE,noGlobalPruning= TRUE)
plot(m3)
p3 <-predict(m3,newdata=wine_test[,-13])
wine$output <- factor(wine$output)
summary(wine$output)
set.seed(234)
index_w1 <- sample(1:nrow(wine_train), 4000, replace = TRUE)
rr_w1 <- wine[index_w1, ]
m_w1 <- C5.0(x=rr_w1[,-13], y=rr_w1$output,earlyStopping = FALSE,noGlobalPruning= TRUE)
plot(m_w1)
p_w1 <-predict(m_w1,newdata=wine_test[,-13])
p_w1
CrossTable(wine_test$output, p_w1,
prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
dnn = c('actual quality', 'predicted quality'))
table(p_w1, wine_test$output)
library(gmodels)
CrossTable(wine_test$output, p_w1,
prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
dnn = c('actual quality', 'predicted quality'))
set.seed(234)
index_w1 <- sample(1:nrow(wine_train), 4000, replace = TRUE)
rr_w1 <- wine[index_w1, ]
m_w1 <- C5.0(x=rr_w1[,-c(12,13)], y=rr_w1$output,earlyStopping = FALSE,noGlobalPruning= TRUE)
plot(m_w1)
p_w1 <-predict(m_w1,newdata=wine_test[,-c(12,13)])
p_w1
table(p_w1, wine_test$output)
CrossTable(wine_test$output, p_w1,
prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
dnn = c('actual quality', 'predicted quality'))
rm(list = ls())
iris_rand <- iris[order(runif(150)), ]
iris_train <- iris_rand[1:140,]
iris_test <- iris_rand[141:150,]
set.seed(123)
index <- sample(1:nrow(iris_train), 140, replace = TRUE)
url <- 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv'
wine <- read.csv2(url)
source('C:/Users/Student/Documents/SYS4021/SourceCode/SPM_Panel.R')
wine$output[wine$quality < 6] <- 'bad'
wine$output[wine$quality == 6] <- 'normal'
wine$output[wine$quality > 6] <- 'good'
wine$output <- factor(wine$output)
wine_rand <- wine[order(runif(4898)), ]
wine_train <- wine_rand[1:4000,]
wine_test <- wine_rand[4001:4898,]
set.seed(234)
index_w1 <- sample(1:nrow(wine_train), 4000, replace = TRUE)
rr_w1 <- wine[index_w1, ]
m_w1 <- C5.0(x=rr_w1[,-c(12,13)], y=rr_w1$output,earlyStopping = FALSE,noGlobalPruning= TRUE)
library(C50)
m_w1 <- C5.0(x=rr_w1[,-c(12,13)], y=rr_w1$output,earlyStopping = FALSE,noGlobalPruning= TRUE)
p_w1 <-predict(m_w1,newdata=wine_test[,-c(12,13)])
table(p_w1, wine_test$output)
library(gmodels)
CrossTable(wine_test$output, p_w1,
prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
dnn = c('actual quality', 'predicted quality'))
library(ipred)
mybag <- bagging(quality~ , data = wine_train, nbagg = 25)
names(wine)
mybag <- bagging(quality~fixed.acidity+volatile.acidity+citric.acid+residual.sugar+chlorides+free.sulfur.dioxide+total.sulfur.dioxide+density+pH+sulphates+alcohol, data = wine_train, nbagg = 25)
wine_pred <- predict(mybag, wine_test[,-c(12,13)])
table(wine_pred, wine_test$quality)
table(int(wine_pred), wine_test$quality)
table(as.integer(wine_pred), wine_test$quality)
CrossTable(as.integar(wine_test$quality), pRF,
prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
dnn = c('actual quality', 'predicted quality'))
CrossTable(as.integer(wine_test$quality), pRF,
prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
dnn = c('actual quality', 'predicted quality'))
library(randomForest)
mRF <- randomForest(wine_train[-c(12,13)],wine_train$quality,ntree=500, mtry=sqrt(11))
mRF <- randomForest(wine_train[-c(12,13)],as.integer(wine_train$quality),ntree=500, mtry=sqrt(11))
qualityLM <- lm(quality~.,data = wine[-c(12)])
qualityLM <- lm(quality~.,data = wine[-c(13)])
summary(qualityLM)
max(coefficients(qualityLM))
max(coefficients(qualityLM),na.rm=TRUE)
head(coefficients(qualityLM),na.rm=TRUE)
head(sort(coefficients(qualityLM),decreasing = TRUE),na.rm = TRUE)
head(sort(coefficients(qualityLM),decreasing = TRUE),n=20,na.rm = TRUE)
rm(list = ls())
url <- 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv'
wine <- read.csv(url,sep = ";")
head(wine)
barplot(table(wine$quality))
pairs(wine)
wine <- mutate(wine,quality = ifelse(quality < 6, "bad", ifelse(quality>6,"good","normal")))
wine$quality <- as.factor(wine$quality)
wine_rand <- wine[order(runif(4898)), ]
wine_train <- wine_rand[1:3264,]
wine_test <- wine_rand[3265:4898,]
library(C50)
m <- C5.0(wine_train[,-12],wine_train$quality)
p <- predict(m,wine_test[,-12])
library(gmodels)
CrossTable(wine_test$quality, p,
prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
dnn = c('actual default', 'predicted default'))
library(ipred)
mybag <-bagging(quality~.,data=wine_train)
pbag <-predict(mybag,wine_test[,-12])
mybag2 <- bagging(quality~.,data=wine,coob=TRUE)
library(randomForest)
model <- randomForest(quality ~ . , data = wine_train)
install.packages('gapminder')
library(gapminder)
str(gapminder)
rm(list = ls())
save.image("~/.RData")
summary(gapminder)
load("~/SYS4021/Spam/.RData")
library(forecast)
plot(spam.ts)
acf(spam.ts)
pg.ham<-spec.pgram(ham.ts,spans=9,demean=T,log='no')
pg.spam <- spec.pgram(spam.ts,spans=9, demean=T, log='no')
max.omega.spam <- pg.spam$freq[which(pg.spam$spec==max(pg.spam$spec))]
1/max.omega.spam
pg.ham<-spec.pgram(ham.ts,spans=9,demean=T,log='no')
spam.trend<-lm(spam.ts[1:(length(spam.ts))]~time.spam)
time.spam<-c(1:(length(spam.ts)))
spam.trend<-lm(spam.ts[1:(length(spam.ts))]~time.spam)
e.ts.spam <- ts(spam.trend$residuals)
plot(e.ts.spam)
acf(e.ts.spam)
pacf(e.ts.spam)
spam.auto <- auto.arima(e.ts.spam, trace=TRUE, stepwise = FALSE)
summary(spam.auto)
spam.arma11.whole <- arima(spam.ts, order=c(1,0,1))
spam.arima111 <- arima(e.ts.spam, order=c(1,1,1))
summary(spam.arima111)
AIC(spam.arima111)
ACF(spam.arima111)
acf(spam.arima111)
pacf(spam.arima111)
time.spam<-c(1:(length(spam.ts)))
spam.trend<-lm(spam.ts[1:(length(spam.ts))]~time.spam)
spam.trend<-lm(spam.ts[1:(length(spam.ts)-1)]~time.spam)
spam.trend<-lm(spam.ts[1:(length(spam.ts))]~time.spam)
summary(spam.trend)
e.ts.spam <- ts(spam.trend$residuals)
plot(e.ts.spam)
acf(e.ts.spam)
pacf(e.ts.spam)
spam.auto <- auto.arima(e.ts.spam, trace=TRUE, stepwise = FALSE)
summary(spam.auto)
time.spam<-c(1:(length(spam.ts))-7)
time.spam<-c(1:(length(spam.ts)-7))
spam.trend<-lm(spam.ts[1:(length(spam.ts))]~time.spam)
spam.trend<-lm(spam.ts[1:357]~time.spam)
e.ts.spam <- ts(spam.trend$residuals)
plot(e.ts.spam)
acf(e.ts.spam)
pacf(e.ts.spam)
spam.auto <- auto.arima(e.ts.spam, trace=TRUE, stepwise = FALSE)
summary(spam.auto)
spam.arima111 <- arima(e.ts.spam, order=c(1,1,1))
summary(spam.arima111)
acf(spam.arima111)
pacf(spam.arima111)
acf(spam.arima111$residuals)
pacf(spam.arima111$residuals)
spam.auto1 <- auto.arima(spam.ts[time.spam], trace = TRUE)
summary(spam.auto1)
acf(spam.auto$residuals)
pacf(spam.auto$residuals)
pg.spam <- spec.pgram(spam.ts,spans=9, demean=T, log='no')
max.omega.spam <- pg.spam$freq[which(pg.spam$spec==max(pg.spam$spec))]
1/max.omega.spam
max.omega.spam
acf(e.ts.spam)
par(mfrow=c(1,2))
acf(e.ts.spam, main="ACF of Residuals from spam.trend")
pacf(e.ts.spam,main="PACF of Residuals from spam.trend")
par(mfrow=c(1,1))
pg.spam <- spec.pgram(spam.ts,spans=9, demean=T, log='no')
?spec.pgram
pg.spam <- spec.pgram(spam.ts,spans=9, demean=T, log='no')
max.omega.spam <- pg.spam$freq[which(pg.spam$spec==max(pg.spam$spec))]
max.omega.spam
1/max.omega.spam
1/max.omega.ham
max.omega.ham<-pg.ham$freq[which(pg.ham$spec==max(pg.ham$spec))]
1/max.omega.ham
?tsdiag
tsdiag(spam.arma11,gof.lag=20)
spam.arma11 <- arima(e.ts.spam, order=c(1,0,1))
spam.ar1 <- arima(e.ts.spam, order=c(1,0,0))
spam.arima111 <- arima(e.ts.spam, order=c(1,1,1))
spam.auto <- auto.arima(e.ts.spam, trace=TRUE, stepwise = FALSE)
spam.auto.whole <- auto.arima(spam.ts, trace=TRUE)
spam.arma11.whole <- arima(spam.ts, order=c(1,0,1))
spam.auto1 <- auto.arima(spam.ts[time.spam], trace = TRUE)
tsdiag(spam.arma11,gof.lag=20)
tsdiag(spam.ar1,gof.lag=20)
tsdiag(spam.arima111,gof.lag=20)
tsdiag(spam.auto,gof.lag=20)
tsdiag(spam.auto1,gof.lag=20)
tsdiag(spam.auto, gof.lag=20)
tsdiag(spam.arima111,gof.lag=20)
tsdiag(spam.auto1,gof.lag=20)
load("~/DS4559/Dating/dating.RData")
adaboostRat <- boosting(dec~age_o+race_o+attr+sinc+intel+fun+amb+shar+like+met,data = dd1_train, boos=FALSE, mfinal=20, coeflearn='Freund')
library(adabag)
adaboostRat <- boosting(dec~age_o+race_o+attr+sinc+intel+fun+amb+shar+like+met,data = dd1_train, boos=FALSE, mfinal=20, coeflearn='Freund')
names(dd1_train)
adaboostRat <- boosting(dec~age_o+race_o+attr+sinc+intel+fun+amb+like+met,data = dd1_train, boos=FALSE, mfinal=20, coeflearn='Freund')
adaboostRat$importance[which(adaboostRat$importance>0)]
adaboostRat <- boosting(dec~age_o+race_o+attr+sinc+intel+fun+amb+met,data = dd1_train, boos=FALSE, mfinal=20, coeflearn='Freund')
adaboostRat <- boosting(dec~age_o+race_o+attr+sinc+intel+fun+amb+met,data = dd1_train, boos=FALSE, mfinal=20, coeflearn='Freund')
adaboostRat$importance[which(adaboostRat$importance>0)]
dd1_train$gender
dd1_trainM <- dd1_train[where(dd1_train$gender==1)]
dd1_trainM <- dd1_train[which(dd1_train$gender==1)]
dd1_trainM <- dd1_train %>% filter(gender == 1)
library(tidvyverse)
library(tidyverse)
dd1_trainM <- dd1_train %>% filter(gender == 1)
adaboostRatM <- boosting(dec~age_o+race_o+attr+sinc+intel+fun+amb+met,data = dd1_trainM, boos=FALSE, mfinal=20, coeflearn='Freund')
adaboostRatM$importance[which(adaboostRatM$importance>0)]
dd1_trainF <- dd1_train %>% filter(gender == 0)
adaboostRatF <- boosting(dec~age_o+race_o+attr+sinc+intel+fun+amb+met,data = dd1_trainF, boos=FALSE, mfinal=20, coeflearn='Freund')
adaboostRatF$importance[which(adaboostRatF$importance>0)]
dating_data %>% ggplot(aes(x=career_c,y=match, fill = factor(gender))) + geom_bar(stat = 'identity')
dating_data %>% ggplot(aes(x=career_c,y=match)) + geom_bar(stat = 'identity')
dating_data %>% ggplot(aes(x=career_c,y=match)) + geom_bar(stat = 'identity')+coord_flip()
dating_data %>% ggplot(aes(x=career_c,y=gender)) + geom_bar(stat = 'identity')+coord_flip()
dating_data %>% ggplot(aes(x=career_c,y=factor(gender))) + geom_bar(stat = 'identity')+coord_flip()
dating_data %>% ggplot(aes(x=career_c,y=factor(gender))) + geom_boxplot(stat = 'identity')+coord_flip()
dating_data %>% ggplot(aes(x=career_c,y=gender)) + geom_boxplot(stat = 'identity')+coord_flip()
dating_data %>% ggplot(aes(x=career_c,y=as.numeric(match))) + geom_boxplot(stat = 'identity')+coord_flip()
dating_data %>% ggplot(aes(x=career_c,y=as.numeric(match))) + geom_bar(stat = 'identity')+coord_flip()
dating_data %>% ggplot(aes(x=career_c,y=as.numeric(match), fill = as.factor(gender))) + geom_bar(stat = 'identity')+coord_flip()
dating_data %>% ggplot(aes(x=career_c,y=as.numeric(match), fill = as.factor(gender))) + geom_bar(stat = 'identity', position = 'dodge')+coord_flip()
apply(dating_data,2,pMiss)
##Let's see how much of the dating data is NA's
sum(is.na(dating_data))/(8378*195)*100
average(dating_data$attr)
mean(dating_data$attr)
mean(dating_data$attr, rm.na = TRUE)
mean(dating_data$attr, na.rm = TRUE)
mean(dating_data$attr_o, na.rm = TRUE)
dating_data %>% filter(gender == 0) %>% summarise(mean(attr))
dating_data %>% filter(gender == 0, !is.na(attr)) %>% summarise(mean(attr))
dating_data %>% filter(gender == 0, !is.na(attr)) %>% summarise(mean(attr_o))
dating_data %>% filter(gender == 0, !is.na(attr_o)) %>% summarise(mean(attr_o))
dating_data %>% filter(gender == 0, !is.na(attr3_1)) %>% summarise(mean(attr3_1))
dating_data %>% filter(gender == 1, !is.na(attr3_1)) %>% summarise(mean(attr3_1))
dating_data %>% filter(!is.na(attr3_1)) %>% summarise(mean(attr3_1))
dating_data %>% filter(!is.na(age)) %>% summarise(mean(age))
dating_data %>% filter(!is.na(race)) %>% summarise(race)
dating_data %>% filter(!is.na(race)) %>% select(race)
dating_data %>% filter(!is.na(race)) %>% summarize(sum(race))
dating_data %>% group_by(race) %>% filter(!is.na(race)) %>% summarize(sum(race))
dating_data %>% group_by(race) %>% filter(!is.na(race)) %>% summarize(sum(race)/sum(dating_data$race))
dating_data %>% group_by(race) %>% filter(!is.na(race)) %>% summarize(sum(race)/sum(dating_data$race, na.rm = TRUE))
dating_data %>% group_by(race) %>% filter(!is.na(race)) %>% summarize(sum(race)/sum(dating_data$race, na.rm = TRUE)*100)
############### Making our models better II: removing correlation ######################
## install a funky new package:
install.packages("corrplot")
corrAttr <- corr(dd1[,c('attr_o','age_o','race_o','attr','sinc','intel','fun','amb','met')])
library(corrPlot)
corrAttr <- corr(dd1[,c('attr_o','age_o','race_o','attr','sinc','intel','fun','amb','met')])
library(corrplot)
corrAttr <- corr(dd1[,c('attr_o','age_o','race_o','attr','sinc','intel','fun','amb','met')])
corrAttr <- cor(dd1[,c('attr_o','age_o','race_o','attr','sinc','intel','fun','amb','met')])
plot(corrAttr)
summary(adaboost1,order='hclust')
plot(corrAttr, order = 'hclust')
corrplot(corrAttr, order = 'hclust')
corrAttr <- cor(dd1[,c('attr_o','attr','sinc','intel','fun','amb','met')])
corrplot(corrAttr, order = 'hclust')
corrAttr <- cor(dd1[,c('attr_o','attr','sinc','intel','fun','amb','met','match')])
corrplot(corrAttr, order = 'hclust')
corrAttr <- cor(dd1[,c('attr_o','attr','sinc','intel','fun','amb','met','as.numeric(match)')])
corrAttr <- cor(dd1[,c('attr_o','attr','sinc','intel','fun','amb','met','dec')])
corrAttr <- cor(dd1[,c('attr_o','attr','sinc','intel','fun','amb','met','race_o','age_o')])
corrplot(corrAttr, order = 'hclust')
dd1_test[12]
names(dd1_test)
p1Rat <- predict(adaboostRat,dd1_test[-84])
RC1Rat <- roc(dd1_test$dec,p1Rat$prob[,2])
plot(RC1Rat, legacy.axes=TRUE)
auc(RC1Rat)
library(pROC)
p1Rat <- predict(adaboostRat,dd1_test[-84])
RC1Rat <- roc(dd1_test$dec,p1Rat$prob[,2])
plot(RC1Rat, legacy.axes=TRUE)
auc(RC1Rat)
adaboostRatM <- boosting(dec~age_o+race_o+attr+sinc+intel+fun+amb+met,data = dd1_trainM, boos=FALSE, mfinal=20, coeflearn='Freund')
adaboostRatM$importance[which(adaboostRatM$importance>0)]
p1RatM <- predict(adaboostRat,dd1_testM[-84])
RC1RatM <- roc(dd1_testM$dec,p1RatM$prob[,2])
plot(RC1RatM, legacy.axes=TRUE)
auc(RC1RatM)
dd1_testM <- dd1_test %>% filter(gender == 1)
dd1_testF <- dd1_test %>% filter(gender == 0)
p1RatM <- predict(adaboostRat,dd1_testM[-84])
RC1RatM <- roc(dd1_testM$dec,p1RatM$prob[,2])
plot(RC1RatM, legacy.axes=TRUE)
auc(RC1RatM)
p1RatF <- predict(adaboostRat,dd1_testF[-84])
RC1RatF <- roc(dd1_testF$dec,p1RatF$prob[,2])
plot(RC1RatF, legacy.axes=TRUE)
auc(RC1RatF)
plot(RC1Rat, legacy.axes = TRUE)
lines(RC1RatM)
lines(RC1RatM, col = 'Blue')
lines(RC1RatF, col = 'Orange')
plot(RC1Rat, legacy.axes = TRUE, col = 'Green')
lines(RC1RatM, col = 'Blue')
lines(RC1RatF, col = 'Orange')
corrAttr <- cor(dd1[,c('attr_o','attr','sinc','intel','fun','amb','met','race','age')])
corrplot(corrAttr, order = 'hclust')
adaboostRat$importance[which(adaboostRat$importance>0)]
##Males and Females
adaboostRat <- boosting(dec~age+race_o+attr+sinc+intel+fun+amb+met,data = dd1_train, boos=FALSE, mfinal=20, coeflearn='Freund')
##Males and Females
adaboostRat <- boosting(dec~age+race+attr+sinc+intel+fun+amb+met,data = dd1_train, boos=FALSE, mfinal=20, coeflearn='Freund')
adaboostRat$importance[which(adaboostRat$importance>0)]
##Just males
adaboostRatM <- boosting(dec~age+race+attr+sinc+intel+fun+amb+met,data = dd1_trainM, boos=FALSE, mfinal=20, coeflearn='Freund')
##Males and Females
adaboostRat <- boosting(dec~age_o+race_o+attr+sinc+intel+fun+amb+met,data = dd1_train, boos=FALSE, mfinal=20, coeflearn='Freund')
adaboostRat$importance[which(adaboostRat$importance>0)]
adaboostRatM$importance[which(adaboostRatM$importance>0)]
##Just males
adaboostRatM <- boosting(dec~age_o+race_o+attr+sinc+intel+fun+amb+met,data = dd1_trainM, boos=FALSE, mfinal=20, coeflearn='Freund')
adaboostRatF$importance[which(adaboostRatF$importance>0)]
install.packages('feedeR')
library(feedeR)
feed.extract('https://www.amazon.com/gp/rss/bestsellers/beauty/ref=zg_bs_beauty_rsslink')
am_beauty <- feed.extract('https://www.amazon.com/gp/rss/bestsellers/beauty/ref=zg_bs_beauty_rsslink')
am_beaty$items
am_beauty$items
am_beauty$items$title
blogs <- rbind(theBeautyBrains[c('Date','Text')], hudaBeauty[c('Date','Text')], hotBeatyHealth[c('Date','Text')], musings[c('Date','Text')],womenStuff[c('Date','Text')],vivaWoman[c('Date','Text')],trendHunter[c('Date','Text')],temptalia[c('Date','Text')])
setwd('~/GitHub/PIPPY-data')
theBeautyBrains <- read.csv('items_thebeautybrains.com_1.csv')
hudaBeauty <- read.csv('items_hudabeauty.com_1.csv')
hotBeatyHealth <- read.csv('items_www.hotbeautyhealth.com_1.csv')
musings <- read.csv('items_www.musingsofamuse.com_1.csv')
womenStuff <- read.csv('items_www.mywomenstuff.com_1.csv')
vivaWoman <- read.csv('items_www.vivawoman.net_1.csv')
trendHunter <- read.csv('items_www.trendhunter.com_1.csv')
trendHunter2 <- read.csv('items_www.trendhunter.com_2.csv')
trendHunter <- rbind(trendHunter, trendHunter2)
rm(trendHunter2)
temptalia <- read.csv('items_www.temptalia.com_1.csv')
blogs <- rbind(theBeautyBrains[c('Date','Text')], hudaBeauty[c('Date','Text')], hotBeatyHealth[c('Date','Text')], musings[c('Date','Text')],womenStuff[c('Date','Text')],vivaWoman[c('Date','Text')],trendHunter[c('Date','Text')],temptalia[c('Date','Text')])
blogs$Date <- as.Date(blogs$Date)
gBlogs <- which(apply(blogs,1,pMiss)<100)
pMiss <- function(x){sum(is.na(x))/length(x)*100}
gBlogs <- which(apply(blogs,1,pMiss)<100)
rm(gBlogs)
keepers <- which(apply(blogs,1,pMiss)<100)
gBlogs <- blogs[keepers,]
head(gBlogs)
keepers <- which(apply(blogs,1,pMiss)==100)
keepers <- which(apply(blogs,1,pMiss)==1)
data <- airquality
data[4:10,3] <- rep(NA,7)
data[1:5,4] <- NA
data <- data[-c(5,6)]
summary(data)
## The following function calculates the percent of data that is missing.  You
## do not need to know how to write a function.  You may simply use it for future
## purposes:
pMiss <- function(x){sum(is.na(x))/length(x)*100}
## Next we apply the function to the rows (samples) (that is what the '1' indicates
apply(data,1,pMiss)
## Now we apply the function to the columns (features) (that is what the '2' represents)
apply(data,2,pMiss)
## Next we apply the function to the rows (samples) (that is what the '1' indicates
apply(data,1,pMiss)
## Next we apply the function to the rows (samples) (that is what the '1' indicates
apply(blogs,1,pMiss)
pMiss <- function(x){sum(is.na(x))/length(x)*100}
blogs$Date <- as.character(blogs$Date)
blogs$Text <- as.character(blogs$Text)
blogs$Date[blogs$Date==""] <- NA
blogs$Text[blogs$Text==""] <- NA
keepers <- which(apply(blogs,1,pMiss)<100)
gBlogs <- blogs[keepers,]
apply(blogs,1,pMiss)
keepers <- which(apply(blogs,1,pMiss)==100)
keepers <- which(apply(blogs,1,pMiss)>0)
gBlogs <- blogs[keepers,]
keepers <- which(apply(blogs,1,pMiss)==0)
gBlogs <- blogs[keepers,]
blogs$Date <- as.Date(blogs$Date)
blogs <- rbind(theBeautyBrains[c('Date','Text')], hudaBeauty[c('Date','Text')], hotBeatyHealth[c('Date','Text')], musings[c('Date','Text')],womenStuff[c('Date','Text')],vivaWoman[c('Date','Text')],trendHunter[c('Date','Text')],temptalia[c('Date','Text')])
pMiss <- function(x){sum(is.na(x))/length(x)*100}
blogs$Date <- as.character(blogs$Date)
blogs$Text <- as.character(blogs$Text)
blogs$Date[blogs$Date==""] <- NA
blogs$Text[blogs$Text==""] <- NA
apply(blogs,1,pMiss)
keepers <- which(apply(blogs,1,pMiss)==0)
gBlogs <- blogs[keepers,]
gBlogs$Date <- as.Date(gBlogs$Date)
View(gBlogs)
blogs$Date <- as.character(blogs$Date)
blogs$Text <- as.character(blogs$Text)
blogs$Date[blogs$Date==""] <- NA
blogs$Text[blogs$Text==""] <- NA
keepers <- which(apply(blogs,1,pMiss)==0)
length(keepers)
gBlogs <- blogs[keepers,]
keepers <- which(apply(blogs,1,pMiss)<100)
gBlogs <- blogs[keepers,]
keepers <- which(apply(blogs,1,pMiss)==0)
gBlogs <- blogs[keepers,]
View(gBlogs)
blogs$Date[blogs$Date=='None']<-NA
keepers <- which(apply(blogs,1,pMiss)==0)
gBlogs <- blogs[keepers,]
View(gBlogs)
gBlogs$Date <- as.Date(gBlogs$Date)
summary(gBlogs$Date)
keepers <- which(apply(blogs,1,pMiss)==0)
gBlogs <- blogs[keepers,]
keepers <- which(apply(gBlogs,1,pMiss)==0)
gBlogs <- gBlogs[keepers,]
summary(gBlogs$Date)
gBlogs$Date <- as.Date(gBlogs$Date)
keepers <- which(apply(gBlogs,1,pMiss)==0)
gBlogs <- gBlogs[keepers,]
gBlogs$Text = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", gBlogs$Text)
# remove at people
gBlogs$Text = gsub("@\\w+", "", gBlogs$Text)
# remove punctuation
gBlogs$Text = gsub("[[:punct:]]", "", gBlogs$Text)
# remove numbers
gBlogs$Text = gsub("[[:digit:]]", "", gBlogs$Text)
# remove html links
gBlogs$Text = gsub("http\\w+", "", gBlogs$Text)
# remove unnecessary spaces
gBlogs$Text = gsub("[ \t]{2,}", "", gBlogs$Text)
gBlogs$Text = gsub("^\\s+|\\s+$", "", gBlogs$Text)
gBlogs$Text = gsub('???','',gBlogs$Text)
gBlogs$Text = gsub('etc','',gBlogs$Text)
# define "tolower error handling" function
try.error = function(x)
{
# create missing value
y = NA
# tryCatch error
try_error = tryCatch(tolower(x), error=function(e) e)
# if not an error
if (!inherits(try_error, "error"))
y = tolower(x)
# result
return(y)
}
# lower case using try.error with sapply
gBlogs$Text = sapply(gBlogs$Text, try.error)
# remove NAs in gBlogs$Text
gBlogs$Text = gBlogs$Text[!is.na(gBlogs$Text)]
names(gBlogs$Text) = NULL
col=brewer.pal(6,"Dark2")
wordcloud(gBlogs$Text, min.freq=5, scale=c(5,2),rot.per = 0.25,
random.color=T, max.word=100, random.order=F,colors=col)
library(wordcloud)
col=brewer.pal(6,"Dark2")
wordcloud(gBlogs$Text, min.freq=5, scale=c(5,2),rot.per = 0.25,
random.color=T, max.word=100, random.order=F,colors=col)
warnings()
wordcloud(gBlogs$Text, min.freq=50, scale=c(5,2),rot.per = 0.25,
random.color=T, max.word=100, random.order=F,colors=col)
warnings()
wordcloud(gBlogs$Text, min.freq=50, scale=c(5,2),rot.per = 0.25,
random.color=T, max.word=10, random.order=F,colors=col)
blogWords <- gBlogs %>% unnest_tokens(word,Text) %>% anti_join(stop_words)
library(tidyverse)
blogWords <- gBlogs %>% unnest_tokens(word,Text) %>% anti_join(stop_words)
blogWords <- gBlogs %>% dplyr::unnest_tokens(word,Text) %>% anti_join(stop_words)
source('~/Capstone/blogs.R', echo=TRUE)
blogWords <- gBlogs %>% tidytext::unnest_tokens(word,Text) %>% anti_join(stop_words)
blogWords <- gBlogs %>% tidytext::unnest_tokens(word,Text) %>% anti_join(tidytext::stop_words)
ingr <- read.csv('Ingredients.csv')
colnames(ingr) = c('Ingredient')
ingr$Ingredient <- as.character(ingr$Ingredient)
ingr <- ingr %>% tidytext::unnest_tokens(word,Ingredient) %>% anti_join(tidytext::stop_words)
ingr$word <- sapply(ingr$word, try.error)
ingrWords <- inner_join(blogWords,ingr,by='word')
wordcloud(ingrWords$word, min.freq=50, scale=c(5,2),rot.per = 0.25,
random.color=T, max.word=10, random.order=F,colors=col)
wordcloud(ingrWords$word, min.freq=25, scale=c(5,2),rot.per = 0.25,
random.color=T, max.word=100, random.order=F,colors=col)
wordcloud(ingrWords$word, min.freq=25,
random.color=T, max.word=100, random.order=F,colors=col)
wordcloud(ingrWords$word, min.freq=25,
random.color=T, max.word=200, random.order=F,colors=col)
head(ingrWords)
ingr1 <- ingr %>% tidytext::unnest_tokens(word,Ingredient) %>% anti_join(tidytext::stop_words)
ingr1$word <- sapply(ingr1$word, try.error)
ingrWords <- inner_join(blogWords,ingr1,by='word')
wordcloud(ingrWords$word, min.freq=25,
random.color=T, max.word=200, random.order=F,colors=col)
write.csv(ingrWords,'ingrWords.csv')
save.image("~/Capstone/blogs.RData")
